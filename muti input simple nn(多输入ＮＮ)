import torch
import torch.nn as nn
from torch.autograd import Variable
import numpy as np
import matplotlib.pyplot as pl
x=np.arange(0,100,dtype=np.float32).reshape([100,1])
y=x*2+10
pl.figure(1)
pl.plot(x,y,'r.')
pl.show()
X=Variable(torch.Tensor(x))
X_=Variable(torch.Tensor(np.arange(0,1,0.01).reshape([100,1])))
Y=Variable(torch.Tensor(y))


class NN(nn.Module):
    def __init__(self):
        super(NN,self).__init__()
        self.layer1=nn.Sequential(nn.Linear(1,3),nn.ReLU())
        self.layer2=nn.Sequential(nn.Linear(3,3),nn.ReLU())
        self.layer3=nn.Sequential(nn.Linear(3,1))

        self.fen1=nn.Linear(1,3)
        self.fen2=nn.Linear(1,3)
        self.combine=nn.Linear(3,1)
    def forward(self, x,x_):  #多输入网络(组合网雏形)
        output1=self.layer1(x)
        output2=self.layer2(output1)
        output3=self.layer3(output2)
        outputleft=self.fen1(output3)
        outputright=self.fen2(x_)
        output=outputleft+outputright
        output=self.combine(output)

        return output
combine_net=NN()
loss=nn.MSELoss()
train=torch.optim.Adam(combine_net.parameters(),lr=0.01)
for i in range(1000):
    #forward
    y=combine_net.forward(X,X_)
    #loss function
    Loss=loss(y,Y)  #错误原因可能需要交换位置Ｙ与ｙ的位置:预测结果放在前面
    # 错误nn criterions don't compute the gradient w.r.t. targets - please mark these variables as volatile or not requiring gradients
    print("Loss:",Loss)
    #grad
    train.zero_grad()
    #backword
    Loss.backward()
    #train
    train.step()
test=combine_net(X,X_)
pl.figure(2)
print(test.data.numpy())
pl.plot(x,test.data.numpy(),'g-')
pl.show()




